Title: WIP article for Victoria Logs
Date: 2025-03-31
Category: Knowledge Base
Tags: logs

# Data preparation for Elasticsearch and Victoria Logs

### Setup
- Elasticsearch: Assume you already have a cluster with 3 nodes!
- VictoriaLogs: docker compose for the best xD
```
services:
  victorialogs:
    image: victoriametrics/victoria-logs:v1.17.0-victorialogs
    container_name: victorialogs
    command:
      - "--storageDataPath=/vlogs"
      - "--httpListenAddr=:9428"
      - "--loggerLevel=INFO"
      - "--retentionPeriod=30d"  # Retain logs for 30 days
    volumes:
      - /data/vlogs-data:/vlogs
    ports:
      - "9428:9428"

volumes:
  vlogs-data:
```

### Elasticsearch

- Generate nginx access logs with 3Gb size for an example via python script
```python
import random
import time

log_formats = [
    '127.0.0.1 - - [27/Mar/2025:15:19:14 +0000] "GET /index.html HTTP/1.1" 200 1024 "-" "Mozilla/5.0"',
    '127.0.0.1 - - [27/Mar/2025:15:19:14 +0000] "POST /form HTTP/1.1" 302 512 "-" "Mozilla/5.0"',
    '127.0.0.1 - - [27/Mar/2025:15:19:14 +0000] "GET /image.png HTTP/1.1" 404 256 "-" "Mozilla/5.0"',
]

def generate_log_entry():
    return random.choice(log_formats)

def main(filename, size_in_gb):
    size_in_bytes = size_in_gb * 1024 * 1024 * 1024
    with open(filename, 'w') as f:
        while f.tell() < size_in_bytes:
            f.write(generate_log_entry() + '\n')
            if f.tell() % (1024 * 1024) == 0:
                print(f"Generated {f.tell() / (1024 * 1024)} MB")

if __name__ == "__main__":
    main("nginx_access.log", 3)
```

- Write log to Elasticsearch (This is not the good way but ok for demo xD). And you need to create index first

```
PUT nginx_access_log
{
  "settings": {
    "index": {
      "number_of_shards": 6,  
      "number_of_replicas": 1 
    }
  }
}
```

```python
import requests
import json

def parse_log_line(line):
    parts = line.split(' ')
    log_entry = {
        "remote_host": parts[0],
        "remote_user": "-",
        "timestamp": parts[3].replace('[', '') + ' ' + parts[4].replace(']', ''),
        "request": parts[5] + ' ' + parts[6] + ' ' + parts[7],
        "status": int(parts[8]),
        "bytes_sent": int(parts[9]),
        "referrer": parts[10].replace('"', ''),
        "user_agent": ' '.join(parts[11:]).replace('"', '')
    }
    return log_entry

def send_bulk_data(bulk_data, index_name):
    bulk_payload = ""
    for item in bulk_data:
        bulk_payload += json.dumps({"index": {"_index": index_name}}) + "\n"
        bulk_payload += json.dumps(item) + "\n"
    
    response = requests.post(
        'http://my-es-ip:9200/_bulk',
        data=bulk_payload,
        headers={'Content-Type': 'application/x-ndjson'},
        auth=('elastic', 'password')
    )

def main():
    bulk_data = []
    batch_size = 49020  # Adjust the batch size to fit 10MB bulk request
    index_name = "nginx_access_log"  # Set the index name
    with open('nginx_access.log', 'r') as f:
        for line in f:
            log_entry = parse_log_line(line)
            bulk_data.append(log_entry)

            if len(bulk_data) >= batch_size:
                send_bulk_data(bulk_data, index_name)
                bulk_data = []

    # Send remaining data
    if bulk_data:
        send_bulk_data(bulk_data, index_name)
        print("Finished!")

if __name__ == "__main__":
    main()
```

- Run script `nginx_to_elasticsearch.py`: `python nginx_to_elasticsearch.py`

### Victoria Logs
- Script to send: `nginx_to_victoria_logs_with_batch_size.py`
```python
import requests
import time
import json
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

def parse_log_line(line):
    parts = line.split(' ')
    log_entry = {
        "remote_host": parts[0],
        "remote_user": "-",
        "timestamp": parts[3].replace('[', '') + ' ' + parts[4].replace(']', ''),
        "request": parts[5] + ' ' + parts[6] + ' ' + parts[7],
        "status": int(parts[8]),
        "bytes_sent": int(parts[9]),
        "referrer": parts[10].replace('"', ''),
        "user_agent": ' '.join(parts[11:]).replace('"', ''),
        "_msg": line.strip()  # Include the original log line as the _msg field
    }
    return log_entry

def convert_to_victoria_logs(log_entry):
    timestamp = int(time.mktime(time.strptime(log_entry['timestamp'], '%d/%b/%Y:%H:%M:%S %z')))
    return {
        "remote_host": log_entry["remote_host"],
        "remote_user": log_entry["remote_user"],
        "timestamp": timestamp,
        "request": log_entry["request"],
        "status": log_entry["status"],
        "bytes_sent": log_entry["bytes_sent"],
        "referrer": log_entry["referrer"],
        "user_agent": log_entry["user_agent"],
        "_msg": log_entry["_msg"]
    }

def send_to_victoria_logs(victoria_log_entries):
    try:
        response = requests.post(
            'http://my-vlog-ip:9428/insert/jsonline',
            data='\n'.join(json.dumps(entry) for entry in victoria_log_entries),
            headers={'Content-Type': 'application/json'}
        )
        response.raise_for_status()  # Raise an exception for HTTP errors
        return True
    except requests.exceptions.RequestException as e:
        logging.error(f"Error sending log entries: {e}")
        return False

def main():
    success_count = 0
    error_count = 0
    batch_size = 10000  # Default batch size
    batch = []

    with open('nginx_access.log', 'r') as f:
        for line in f:
            log_entry = parse_log_line(line)
            victoria_log_entry = convert_to_victoria_logs(log_entry)
            batch.append(victoria_log_entry)

            if len(batch) >= batch_size:
                if send_to_victoria_logs(batch):
                    success_count += len(batch)
                    logging.info(f"Successfully sent {len(batch)} log entries.")
                else:
                    error_count += len(batch)
                batch = []

    # Send remaining log entries
    if batch:
        if send_to_victoria_logs(batch):
            success_count += len(batch)
            logging.info(f"Successfully sent {len(batch)} log entries.")
        else:
            error_count += len(batch)

    logging.info(f"Total successfully sent logs: {success_count}")
    logging.info(f"Total failed logs: {error_count}")

if __name__ == "__main__":
    main()
```

# Comparison

### Elasticsearch disk Usage:
- Query: `GET _cat/indices/nginx_access_log?s=index&v`
- Output:
```
health status index            uuid                   pri rep docs.count docs.deleted store.size pri.store.size
green  open   nginx_access_log eqxEG2RST0-lCTrHfu1Spg   6   1   34147211            0      1.7gb          887mb
```

### VictoriaLogs disk Usage:
- Count total document: `* | stats count() as logs_total`

![alt text](images/2025/03/30th_2.png)

- I used exporter built-in and display it with this dashboard: https://grafana.com/grafana/dashboards/22084-victorialogs-single-node/
![alt text](images/2025/03/30th_1.png)

### Compare
I see my test is not optimized, it can be optimized more to save more space for logging.

- Elasticsearch: 887Mb
- VictoriaLogs: 137Mb

So for this quick test, the victoriaLogs use space to storage log less than ~6.5 times than Elasticsearch.

### My option
- VictoriaLogs used less disk than Elasticsearch, could be up to 15x in some specific scenario.
- VictoriaLogs insert data faster and use less CPU than Elasticsearch.
- VictoriaLogs simple read is faster than Elasticsearch, but when it comes to query with AGG, Elasticsearch win xD
- And many other things i don't know yet at this time when i'm writing this article.
- VictoriaLogs doesn't has cluster mode yet but it can be fix with vmauth and multiple single VictoriaLogs.
example: 2 VictoriaLogs instance in 1 cluster. Use vmauth utility to distribute search query.
- In my company, we have used 4 VictoriaLogs instance with 1 vmauth, 2 instances for 10 K8S cluster logging and 2 instances for Kong - APIGateway with avg request about 20k -> 80k RPS and it has been tested for 3 month before we drop Elasticsearch.

# Conclusion
- Good for logging solution where user need a place to check logs of their container in K8S environment with multiple cluster


# Ref:
- [https://news.ycombinator.com/item?id=42859016](https://news.ycombinator.com/item?id=42859016)
- [https://itnext.io/how-do-open-source-solutions-for-logs-work-elasticsearch-loki-and-victorialogs-9f7097ecbc2f](https://itnext.io/how-do-open-source-solutions-for-logs-work-elasticsearch-loki-and-victorialogs-9f7097ecbc2f)
- [https://www.influxdata.com/comparison/elasticsearch-vs-victoria/](https://www.influxdata.com/comparison/elasticsearch-vs-victoria/)